# -*- coding: utf-8 -*-
"""Dog-breed-identification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pw29Zr1l6ZlozIwIeIOGwGkbCm1Dt7Rw
"""



"""# **üê∂End-to-end  Multi-class Dog Breed Classification **

* Name: Suprasanna V Gunaga
* BE Information Science & Engineering ,RNSIT Bengaluru (2021-2025)

"""

!unzip "drive/MyDrive/Dog vision/dog-breed-identification.zip" -d "drive/MyDrive/DogVision/"

#Import TensorFlow
import tensorflow as tf
print("TF version:", tf.__version__)

#Importing tools
import tensorflow_hub as hub
print("TF HUB version:", hub.__version__)

print("GPU", "Available" if tf.config.list_physical_devices("GPU") else "not available")

"""##Getting data ready (Turning into Tensors)

---


"""

#Check out the labels of data
import pandas as pd
labels_csv=pd.read_csv("drive/MyDrive/DogVision/labels.csv")
print(labels_csv.describe())
print(labels_csv.head())

labels_csv.head()

labels_csv["breed"].value_counts()

labels_csv["breed"].value_counts().plot.bar(figsize=(20,10))

from IPython.display import Image
Image("drive/MyDrive/DogVision/train/001513dfcb2ffafc82cccf4d8bbaba97.jpg")



"""# Getting images and their labels
Getting a list of all images file pathnames
"""

#creating pathnames from image ID's
filenames=["drive/MyDrive/DogVision/train/"+fname+ ".jpg" for fname in labels_csv["id"]]

filenames[:10]

import os
if len(os.listdir("drive/MyDrive/DogVision/train/"))==len(filenames):
  print("Matched")
else:
  print("Not matched")
print(len(os.listdir("drive/MyDrive/DogVision/train/")))
print(len(filenames))
print(len(os.listdir("drive/MyDrive/DogVision/test/")))



"""# Turning data labels into numbers

"""

import numpy as np
labels=labels_csv["breed"].to_numpy()
labels,len(labels)

len(labels)

# Finding the unique label value
unique_breeds=np.unique(labels)
len(unique_breeds),unique_breeds[0:5]

#Turning every label into boolean array
bool_labels=[label == unique_breeds for label in labels]
bool_labels[:2]

len(bool_labels)

# turning boolean array into integers
print(labels[0])
print(np.where(unique_breeds==labels[0]))
print(bool_labels[0].argmax())
print(bool_labels[0].astype(int))

print(labels[2])
print(bool_labels[2].astype(int))



"""# Creating validation set"""

X=filenames
y=bool_labels

len(filenames),len(bool_labels)

#Setting number of images to use for experimenting
NUM_IMAGES=1000 #@param {type:"slider",min:1000,max:10000}

from sklearn.model_selection import train_test_split

X_train,X_val,y_train,y_val=train_test_split(X[:NUM_IMAGES],
                                             y[:NUM_IMAGES],
                                             test_size=0.2,
                                             random_state=42)
len(X_train),len(y_train),len(X_val),len(y_val)

X_train[:5],y_train[:2]



"""# Peprocessing Images (Turning images into Tensors)
Steps:

1.Takes an image filename as input.

2.Uses TensorFlow to read the file and save it to a variable, image.

3.Turn our image (a jpeg file) into Tensors.

4.Normalize image(Convert colour channel values from 0-255 to 0-1)

5.Resize the image to be of shape (224, 224).

6.Return the modified image.
"""

# Converting images into numpy array
from matplotlib.pyplot import imread
image=imread(filenames[40])
image.shape

image

image.max(), image.min()

tf.constant(image)[:2]

#Steps

# Defining Image size
IMG_SIZE= 224

# Creating a function for preprocessing images
def process_image(image_path,img_size=IMG_SIZE):
  """
  Takes an image file path and turns the image into a Tensor.
  """
  #Read in an image file
  image=tf.io.read_file(image_path)
  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)
  image = tf.image.decode_jpeg(image, channels=3)
  # Convert the colour channel values from 0-225 values to 0-1 values
  image = tf.image.convert_image_dtype(image, tf.float32)
  # Resize the image to our desired size (224, 244)
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])
  return image



"""# Turning data into baches"""

# Create a simple function to return a tuple (image, label)
def get_image_label(image_path, label):
  """
  Takes an image file path name and the associated label,
  processes the image and returns a tuple of (image, label).
  """
  image = process_image(image_path)
  return image, label

#Defining the batch size
BATCH_SIZE=32
#Creating a function to turn data into batches
def create_data_batches(X,y=None,batch_size=BATCH_SIZE,valid_data=False,test_data=False):
  """
  Creates batches of data out of image (X) and label (y) pairs.
  Shuffles the data if its training data but doesn't shuffle if its validation data
  Also accepts test data as input(no labels)
  """
  #If the data is a test dataset, we probably dont have labels
  if test_data:
    print("Creating test data batches")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X)))
    data_batch=data.map(process_image).batch(BATCH_SIZE)
    return data_batch

  # If its a valid dataset ,we dont need to shuffle it
  if valid_data:
    print("Creating tvalidation data batches")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(y)))
    data_batch=data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    print("Creating training data batches")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(y)))
    #shuffing pathnames and labels before mapping image processor function is faster than shuffling
    data=data.shuffle(buffer_size=len(X))
    #Creating (image,label) tuples (this also turns the image path into a preprocessed image)
    data = data.map(get_image_label)
    #Turning the training data into batches
    data_batch=data.batch(BATCH_SIZE)
  return data_batch

# Creating training and validation data batches
train_data =create_data_batches(X_train,y_train)
val_data=create_data_batches(X_val, y_val, valid_data=True)

# Checking out the different attributes of our data batches
train_data.element_spec, val_data.element_spec



"""# Visualizing Data Batches"""

import matplotlib.pyplot as plt

# Create a function for viewing images in a data batch
def show_25_images(images, labels):
  """
  Displays a plot of 25 images and their labels from a data batch
  """
  #Setup the figure
  plt.figure(figsize=(10,10))
  # Loop through 25 (for displaying 25 images)
  for i in range(25):
    # Creating subplots (5 rows, 5 coloumns)
    ax=plt.subplot(5,5,i+1)
    # Displaying an image
    plt.imshow(images[i])
    # Add the images label as the title
    plt.title(unique_breeds[labels[i].argmax()])
    # Turning the grid lines of

train_data

# Visualize the data in a training batch
train_images, train_labels =next(train_data.as_numpy_iterator())
show_25_images(train_images, train_labels)

# Visualizing the validation set
val_images , val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)



"""# Building a model

The input shape (images, in the form of Tensors) to our model.

The output shape (image labels, in the form of Tensors) of our model.

The URL of the model we want to use.
"""

# Setup input shape to the model
INPUT_SHAPE=[None, IMG_SIZE,IMG_SIZE,3] #batch ,height,width,colour channels

# Set up output shape of our model
OUTPUT_SHAPE=len(unique_breeds)

# Set up model URL from TensorFlow Hub
# MODEL_URL="https://www.kaggle.com/models/google/mobilenet-v2/frameworks/TensorFlow2/variations/035-128-classification/versions/2"
MODEL_URL="https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"

"""Putting inputs and outputs into a keras deep learning model

Creating a Function which

* Takes the input shape,output shape and the model we've chosen as parameters
* Defines the layers in a keras model in sequential fashion
* Compiles the model
* Builds the model
* Returns the model
"""

#Creating a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE,output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building model with:", MODEL_URL)

  #Set up the model layers
  model=tf.keras.Sequential([
      hub.KerasLayer(MODEL_URL),# input_layer
      tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                            activation="softmax") # Output layer
  ])

  # Compile the model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=["accuracy"]
  )
  # Build the model
  model.build(INPUT_SHAPE)

  return model

model= create_model()
model.summary()



"""# Creating Callbacks

Callbacks are helps functions a model can be use during training to do such things as save its progress, check its progress or stop training early if a model stops improving

TensorBoard callback
* Load the TensorBoard notebook extension
* Create a TensorBoard callback which is able to save logs to a directory and pass it to our models fit() function.
* Visualize our models training logs with the %tensorboard magic function

"""

# Commented out IPython magic to ensure Python compatibility.
# Load TensorBoard notebook extension
# %load_ext tensorboard

import datetime

#creating a function to build a TensorBoard callback
def create_tensorboard_callback():
  # Create a log directory for storing TensorBoard logs
  logdir=os.path.join("drive/MyDrive/DogVision/logs",
                      # make it so the logs get tracked whenever we run an experiment
                      datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

"""# Early stopping Callback
Early stopping callback helps to stop model from overfitting by stopping training if a certain evaluation metric stops improving.
"""

# Create early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuarcy",patience=3)





"""# Training a model (On subset of data)
* Train on 1000 images

"""

NUM_EPOCHS=100  #@param {type:"slider",min:10,max:100,step=10}

"""Creating a function which trains a model

* Creating a model using create_model()
* Setup a Tensorboard callback using create_tensorboard_callback
* Call the fit() function on our model passing it the training data, validation data ,number of epochs to train for (NUM_EPOCHS) and the callbacks we'd like to use
* Return the model
"""

# Building a function to train and return a trained model
def train_model():
  """
  Trains a given model and returns the trained model
  """
  # Create a model
  model=create_model()

  # craete new TensoeBoard session everytime we train a model
  tensorboard= create_tensorboard_callback()

  # Fit the model to the data passing it the callbacks we created
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard, early_stopping])
  #Return the fitted model
  return model

#Fit the model to the data
model=train_model()



"""# Making and Evaluating predictions using training model"""

val_data

# Make predictions on the validation data (notused to train on)
predictions= model.predict(val_data,verbose=1)
predictions

predictions.shape

# First Predictions
index=0
print(predictions[0])
print(f"Max value (probability of prediction): {np.max(predictions[index])}")
print(f"Sum: {np.sum(predictions[index])}")
print(f"Max index: {np.argmax(predictions[index])}")
print(f"Predicted  label: {unique_breeds[np.argmax(predictions[index])]}")

# Turning the probability to respective label

def get_pred_label(prediction_probabilities):
  """
  Turns an array of predictions probabilities into a label
  """
  return unique_breeds[np.argmax(prediction_probabilities)]

  #Getting a predicted label based on an array of prediction probabilities
pred_label = get_pred_label(predictions[81])
pred_label

"""Validation data is in a batch dataset, we will have to unbatchify it to make predictions on the validation images and the compare those predictions to the validation labels( truth value)"""

val_data

# Create a function to unbatch a batch dataset
def unbatchify(data):
  """
  Takes a batched dataset of (image,label) Tensor and returns seperate arrays of images and labels.
  """
  images=[]
  labels=[]
  # Loop through unbatched data
  for image,label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breeds[np.argmax(label)])
  return images, labels

# Unbatchify the validation data
val_images,val_labels= unbatchify(val_data)
val_images[0], val_labels[0]

"""Creating a function to visualizing
* Takes an array of probabilities ,an array of truth labels and array of images and integer
* Convert the prediction probabilities to a predicted label
* Plot the predicted label, its predicted probability the truth label and target image on a single plot

"""

def plot_pred(prediction_probabilities,labels,images,n=23):
  """
  View the prediction,ground truth and image for sample n
  """
  pred_prob,true_label,image=prediction_probabilities[n],labels[n],images[n]

  #Get the pred label
  pred_label =get_pred_label(pred_prob)

  # Plot image & remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  #change the colour of the title depending on if the prediction is right or wrong
  if pred_label==true_label:
    color="green"
  else:
    color="red"


  # Change plot title to be predicted, probability of prediction and truth label
  plt.title("{}{:2.0f}% {}".format(pred_label ,
                                   np.max(pred_prob)*100,true_label),color=color)

plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images)

"""Creating Function to visualize Models top predictions

* Taking an input of predictions probabilities' arrray and a ground truth array and an integer
* Finding the predictions using get_pred-label()
* Prediction probabilities indexes
* Prediction probabilities values
* predicting labels

"""

import matplotlib.pyplot as plt

def plot_pred_conf(prediction_probabilities,labels,n=1):

  """
  Plus the top 10 highest prediction confidences along with the label for sample n.
  """
  pred_prob,true_label=prediction_probabilities[n],labels[n]

  # Get the predicted label
  pred_label=get_pred_label(pred_prob)

  #Finding the top 10 prediction confidence indexes
  top_10_pred_indexes= pred_prob.argsort()[-10:][::-1]

  # Finding the top 10 prediction confidense values
  top_10_pred_values= pred_prob[top_10_pred_indexes]

  # Finding the top 10 prediction labels
  top_10_pred_labels= unique_breeds[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                            top_10_pred_values,
                            color= "grey")
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation="vertical")
  # Change color of true label
  if np.isin(true_label,top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels==true_label)].set_color("green")
  else:
    pass

import matplotlib.pyplot as plt

plot_pred_conf(prediction_probabilities=predictions,
               labels=val_labels,
               n=9)



"""# Saving and reloading model


"""

def save_model(model, suffix=None):
  """
  Saves a given model in a models directory and appends a suffix (str)
  for clarity and reuse.
  """
  # Create model directory with current time
  modeldir = os.path.join("drive/MyDrive/DogVision/model",
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  model_path = modeldir + "-" + suffix + ".h5" # save format of model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)
  return model_path

def load_model(model_path):
  """
  Loads a saved model from a specified path.
  """
  print(f"Loading saved model from: {model_path}")
  model = tf.keras.models.load_model(model_path,
                                     custom_objects={"KerasLayer":hub.KerasLayer})
  return model

# Save our model trained on 1000 images
save_model(model, suffix="1000-images-Adam")

# Load our model trained on 1000 images
model_1000_images = load_model("drive/MyDrive/DogVision/model/20231204-13551701698115-1000-images-Adam.h5")

"""Compare the two models (the original one and loaded one). We can do so easily using the evaluate() method."""

# Evaluate the pre-saved model
model.evaluate(val_data)

# Evaluate the loaded model
model_1000_images.evaluate(val_data)



"""# Training Model (On the full data)"""

len(X), len(y)

# Turn full training data in a data batch
full_data = create_data_batches(X, y)

# Instantiate a new model for training on the full dataset
full_model = create_model()

# Create full model callbacks

# TensorBoard callback
full_model_tensorboard = create_tensorboard_callback()

# Early stopping callback
# Note: No validation set when training on all the data, therefore can't monitor validation accruacy
full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="accuracy",
                                                             patience=3)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/My\Drive/DogVision/logs

# Fit the full model to the full training data
full_model.fit(x=full_data,
               epochs=NUM_EPOCHS,
               callbacks=[full_model_tensorboard,
                          full_model_early_stopping])



"""# Saving and reloading the full model"""

# Save model to file
save_model(full_model, suffix="all-images-Adam")

# Loadind in the full model
loaded_full_model=load_model("drive/MyDrive/DogVision/model/20231204-14201701699657-all-images-Adam.h5")

"""Making predictions on the test dataset"""

# Get custom image filepaths
test_path = "drive/MyDrive/DogVision/test/"
test_filenames = [test_path + fname for fname in os.listdir(test_path)]

test_filenames[:10]

len(test_filenames)

# Create test data batch
test_data = create_data_batches(test_filenames, test_data=True)

# Make predictions on test data batch using the loaded full model
test_predictions = loaded_full_model.predict(test_data,
                                             verbose=1)

# Check out the test predictions
test_predictions[:10]



"""# Making predictions on custom images"""

# Get custom image filepaths
custom_path = "drive/My Drive/DogVision/dogs/"
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path)]

# Turn custom image into batch (set to test data because there are no labels)
custom_data = create_data_batches(custom_image_paths, test_data=True)

# Make predictions on the custom data
custom_preds = loaded_full_model.predict(custom_data)

# Get custom image prediction labels
custom_pred_labels = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_pred_labels

# Get custom images (our unbatchify() function won't work since there aren't labels)
custom_images = []
# Loop through unbatched data
for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)

# Check custom image predictions
plt.figure(figsize=(10, 10))
for i, image in enumerate(custom_images):
  plt.subplot(1, 3, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_pred_labels[i])
  plt.imshow(image)

